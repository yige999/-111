import feedparser
import httpx
import re
from datetime import datetime, timezone
from typing import List, Optional
from urllib.parse import urlparse, urljoin
import logging

from ..models import RawTool

logger = logging.getLogger(__name__)


class ProductHuntScraper:
    """ProductHunt RSS ä¸“ç”¨æŠ“å–å™¨"""

    def __init__(self):
        self.user_agent = "AutoSaaS-Radar-Bot/1.0"
        self.headers = {
            "User-Agent": self.user_agent,
            "Accept": "application/rss+xml, application/xml, text/xml"
        }
        self.base_url = "https://www.producthunt.com"

    async def fetch_producthunt_tools(self, limit: int = 50) -> List[RawTool]:
        """æŠ“å–ProductHunt RSSä¸­çš„AIå·¥å…·"""
        feed_url = "https://www.producthunt.com/feed"

        try:
            async with httpx.AsyncClient(timeout=30, headers=self.headers) as client:
                response = await client.get(feed_url)
                response.raise_for_status()

                # è§£æRSS
                feed = feedparser.parse(response.content)

                if feed.bozo:
                    logger.warning(f"ProductHunt RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

                tools = []
                for entry in feed.entries[:limit]:
                    tool = self._parse_producthunt_entry(entry)
                    if tool and self._is_ai_related(tool):
                        tools.append(tool)

                logger.info(f"ä»ProductHuntç­›é€‰å‡º {len(tools)} ä¸ªAIå·¥å…·")
                return tools

        except httpx.HTTPError as e:
            logger.error(f"ProductHunt HTTPé”™è¯¯: {e}")
        except Exception as e:
            logger.error(f"ProductHuntæŠ“å–å¤±è´¥: {e}")

        return []

    def _parse_producthunt_entry(self, entry) -> Optional[RawTool]:
        """è§£æProductHunt RSSæ¡ç›®"""
        try:
            title = getattr(entry, 'title', '').strip()
            description = getattr(entry, 'description', getattr(entry, 'summary', '')).strip()
            link = getattr(entry, 'link', '').strip()

            # æ¸…ç†HTMLæ ‡ç­¾
            description = self._clean_html(description)

            if not title or not link:
                return None

            # ProductHuntç‰¹å®šå¤„ç†
            title = self._clean_title(title)
            votes = self._extract_votes_from_ph(entry)

            # æå–æ—¥æœŸ
            date = self._extract_date(entry)

            # æ ‡å‡†åŒ–URL
            link = self._normalize_producthunt_url(link)

            return RawTool(
                tool_name=title,
                description=description[:500],
                votes=votes,
                link=link,
                date=date,
                category=""
            )

        except Exception as e:
            logger.error(f"è§£æProductHuntæ¡ç›®å¤±è´¥: {e}")
            return None

    def _is_ai_related(self, tool: RawTool) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAIç›¸å…³å·¥å…·"""
        ai_keywords = [
            'AI', 'artificial intelligence', 'GPT', 'ChatGPT', 'OpenAI',
            'machine learning', 'ML', 'deep learning', 'neural network',
            'automation', 'bot', 'assistant', 'claude', 'gemini',
            'ç”Ÿæˆå¼', 'äººå·¥æ™ºèƒ½', 'æ™ºèƒ½', 'è‡ªåŠ¨', 'åŠ©æ‰‹'
        ]

        text_to_check = (tool.tool_name + ' ' + tool.description).lower()

        return any(keyword.lower() in text_to_check for keyword in ai_keywords)

    def _clean_title(self, title: str) -> str:
        """æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ProductHuntå‰ç¼€"""
        # ç§»é™¤å¸¸è§çš„ProductHuntå‰ç¼€
        prefixes_to_remove = [
            r'^Product Hunt\s*-\s*',
            r'^PH\s*-\s*',
            r'^\[Product Hunt\]\s*',
        ]

        for prefix in prefixes_to_remove:
            title = re.sub(prefix, '', title, flags=re.IGNORECASE)

        return title.strip()

    def _extract_votes_from_ph(self, entry) -> int:
        """ä»ProductHuntæ¡ç›®ä¸­æå–æŠ•ç¥¨æ•°"""
        # ProductHunt RSSå¯èƒ½åœ¨tagsæˆ–summaryä¸­åŒ…å«æŠ•ç¥¨æ•°
        description = getattr(entry, 'description', getattr(entry, 'summary', ''))

        # æŸ¥æ‰¾æŠ•ç¥¨æ•°æ¨¡å¼
        vote_patterns = [
            r'(\d+)\s*votes?',
            r'(\d+)\s*upvotes?',
            r'ğŸ‘\s*(\d+)',
            r'â†‘\s*(\d+)',
        ]

        for pattern in vote_patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue

        return 0

    def _normalize_producthunt_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–ProductHunt URL"""
        if not url:
            return ""

        # ç§»é™¤UTMå‚æ•°å’Œæ¨èå‚æ•°
        url = re.sub(r'[?&]utm_[^&]*', '', url)
        url = re.sub(r'[?&]ref=[^&]*', '', url)

        # ç¡®ä¿æ˜¯å®Œæ•´URL
        if not url.startswith(('http://', 'https://')):
            url = urljoin(self.base_url, url)

        return url

    def _clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text).strip()

    def _extract_date(self, entry) -> datetime:
        """æå–å‘å¸ƒæ—¥æœŸ"""
        date_fields = ['published_parsed', 'updated_parsed']

        for field in date_fields:
            if hasattr(entry, field):
                time_struct = getattr(entry, field)
                if time_struct:
                    try:
                        return datetime(*time_struct[:6], tzinfo=timezone.utc)
                    except (ValueError, TypeError):
                        continue

        return datetime.now(timezone.utc)


async def fetch_producthunt_tools(limit: int = 50) -> List[RawTool]:
    """ä¾¿æ·å‡½æ•°ï¼šæŠ“å–ProductHunt AIå·¥å…·"""
    scraper = ProductHuntScraper()
    return await scraper.fetch_producthunt_tools(limit)