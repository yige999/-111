import feedparser
import httpx
from datetime import datetime, timezone
from typing import List, Optional
import re
from urllib.parse import urljoin, urlparse
import logging

from ..models import RawTool

logger = logging.getLogger(__name__)


class RSScraper:
    """RSS feed æŠ“å–å™¨"""

    def __init__(self, user_agent: str = "AutoSaaS-Radar-Bot/1.0"):
        self.user_agent = user_agent
        self.headers = {
            "User-Agent": user_agent,
            "Accept": "application/rss+xml, application/xml, text/xml"
        }

    async def fetch_feed(self, feed_url: str, timeout: int = 30) -> Optional[List[RawTool]]:
        """æŠ“å–å•ä¸ªRSS feed"""
        try:
            async with httpx.AsyncClient(timeout=timeout, headers=self.headers) as client:
                response = await client.get(feed_url)
                response.raise_for_status()

                # è§£æRSS
                feed = feedparser.parse(response.content)

                if feed.bozo:
                    logger.warning(f"RSSè§£æè­¦å‘Š {feed_url}: {feed.bozo_exception}")

                tools = []
                for entry in feed.entries:
                    tool = self._parse_entry(entry, feed_url)
                    if tool:
                        tools.append(tool)

                logger.info(f"ä» {feed_url} æŠ“å–åˆ° {len(tools)} ä¸ªå·¥å…·")
                return tools

        except httpx.HTTPError as e:
            logger.error(f"HTTPé”™è¯¯æŠ“å– {feed_url}: {e}")
        except Exception as e:
            logger.error(f"æŠ“å–RSS {feed_url} å¤±è´¥: {e}")

        return None

    def _parse_entry(self, entry, feed_url: str) -> Optional[RawTool]:
        """è§£æRSSæ¡ç›®"""
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title = getattr(entry, 'title', '').strip()
            description = getattr(entry, 'description', getattr(entry, 'summary', '')).strip()
            link = getattr(entry, 'link', '').strip()

            # æ¸…ç†HTMLæ ‡ç­¾
            description = self._clean_html(description)

            # éªŒè¯å¿…è¦å­—æ®µ
            if not title or not link:
                return None

            # æå–æ—¥æœŸ
            date = self._extract_date(entry)

            # æå–æŠ•ç¥¨æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            votes = self._extract_votes(entry)

            # æ ‡å‡†åŒ–URL
            link = self._normalize_url(link, feed_url)

            return RawTool(
                tool_name=title,
                description=description[:500],  # é™åˆ¶æè¿°é•¿åº¦
                votes=votes,
                link=link,
                date=date,
                category=""
            )

        except Exception as e:
            logger.error(f"è§£æRSSæ¡ç›®å¤±è´¥: {e}")
            return None

    def _clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        import re
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text).strip()

    def _extract_date(self, entry) -> datetime:
        """æå–å‘å¸ƒæ—¥æœŸ"""
        # å°è¯•å¤šç§æ—¥æœŸå­—æ®µ
        date_fields = ['published_parsed', 'updated_parsed']

        for field in date_fields:
            if hasattr(entry, field):
                time_struct = getattr(entry, field)
                if time_struct:
                    try:
                        return datetime(*time_struct[:6], tzinfo=timezone.utc)
                    except (ValueError, TypeError):
                        continue

        # é»˜è®¤ä½¿ç”¨å½“å‰æ—¶é—´
        return datetime.now(timezone.utc)

    def _extract_votes(self, entry) -> int:
        """æå–æŠ•ç¥¨æ•°"""
        # ä»æè¿°ä¸­æå–å¯èƒ½çš„æŠ•ç¥¨æ•°
        description = getattr(entry, 'description', getattr(entry, 'summary', ''))

        # åŒ¹é…å¸¸è§çš„æŠ•ç¥¨æ•°æ ¼å¼
        vote_patterns = [
            r'(\d+)\s*votes?',
            r'(\d+)\s*upvotes?',
            r'(\d+)\s*ğŸ‘',
            r'(\d+)\s*â™¥',
        ]

        for pattern in vote_patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue

        return 0

    def _normalize_url(self, url: str, base_url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        if not url:
            return ""

        # ç§»é™¤UTMå‚æ•°
        url = re.sub(r'[?&]utm_[^&]*', '', url)
        url = re.sub(r'[?&]ref=[^&]*', '', url)

        # ç¡®ä¿æ˜¯å®Œæ•´URL
        if not url.startswith(('http://', 'https://')):
            base_domain = urlparse(base_url).netloc
            url = urljoin(f"https://{base_domain}", url)

        return url


async def fetch_all_feeds(feed_urls: List[str]) -> List[RawTool]:
    """æŠ“å–æ‰€æœ‰RSS feeds"""
    scraper = RSScraper()
    all_tools = []

    for feed_url in feed_urls:
        logger.info(f"å¼€å§‹æŠ“å–RSS: {feed_url}")
        tools = await scraper.fetch_feed(feed_url)
        if tools:
            all_tools.extend(tools)

    # å»é‡ï¼ˆåŸºäºé“¾æ¥ï¼‰
    seen_links = set()
    unique_tools = []
    for tool in all_tools:
        if tool.link not in seen_links:
            seen_links.add(tool.link)
            unique_tools.append(tool)

    logger.info(f"æ€»å…±æŠ“å–åˆ° {len(unique_tools)} ä¸ªå”¯ä¸€å·¥å…·")
    return unique_tools